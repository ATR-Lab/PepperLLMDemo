diff --git a/app/src/main/java/com/example/peppertest/MainActivity.kt b/app/src/main/java/com/example/peppertest/MainActivity.kt
index 8a0d218..8fe1d3f 100644
--- a/app/src/main/java/com/example/peppertest/MainActivity.kt
+++ b/app/src/main/java/com/example/peppertest/MainActivity.kt
@@ -1,148 +1,77 @@
 package com.example.peppertest
 
 import android.content.Context
+import android.graphics.BitmapFactory
 import android.graphics.Color
 import android.os.Bundle
 import android.util.Log
 import android.view.View
 import android.widget.EditText
 import androidx.appcompat.app.AlertDialog
+import com.aldebaran.qi.Consumer
 import com.aldebaran.qi.Future
+import com.aldebaran.qi.sdk.Qi
 import com.aldebaran.qi.sdk.QiContext
 import com.aldebaran.qi.sdk.QiSDK
 import com.aldebaran.qi.sdk.RobotLifecycleCallbacks
 import com.aldebaran.qi.sdk.builder.AnimateBuilder
 import com.aldebaran.qi.sdk.builder.AnimationBuilder
 import com.aldebaran.qi.sdk.builder.EngageHumanBuilder
+import com.aldebaran.qi.sdk.builder.HolderBuilder
 import com.aldebaran.qi.sdk.builder.SayBuilder
 import com.aldebaran.qi.sdk.`object`.actuation.Animate
 import com.aldebaran.qi.sdk.`object`.actuation.Animation
 import com.aldebaran.qi.sdk.`object`.conversation.Phrase
+import com.aldebaran.qi.sdk.`object`.holder.AutonomousAbilitiesType
+import com.aldebaran.qi.sdk.`object`.holder.Holder
 import com.aldebaran.qi.sdk.`object`.human.EngagementIntentionState
 import com.aldebaran.qi.sdk.`object`.human.Human
 import com.aldebaran.qi.sdk.`object`.humanawareness.EngageHuman
+import com.aldebaran.qi.sdk.`object`.humanawareness.HumanAwareness
+import com.aldebaran.qi.sdk.`object`.human.ExcitementState
+import com.aldebaran.qi.sdk.`object`.human.AttentionState
+import com.aldebaran.qi.sdk.`object`.humanawareness.EngagementPolicy
 import com.aldebaran.qi.sdk.design.activity.RobotActivity
 import com.example.peppertest.camera.PepperCameraManager
 import com.example.peppertest.command.CommandDispatcher
-import com.example.peppertest.websocket.PepperWebSocketClient
+// import com.example.peppertest.websocket.PepperWebSocketClient // Commented out WebSocket import
 import kotlinx.android.synthetic.main.activity_main.*
+import org.json.JSONArray
 import org.json.JSONObject
 import java.util.Timer
 import java.util.TimerTask
 import java.util.concurrent.TimeUnit
+import java.util.concurrent.atomic.AtomicBoolean
 
-class MainActivity : RobotActivity(), RobotLifecycleCallbacks, 
-                     PepperCameraManager.FrameListener,
-                     PepperWebSocketClient.CommandListener,
-                     PepperWebSocketClient.ConnectionStateListener {
-                     
+class MainActivity : RobotActivity(), RobotLifecycleCallbacks {
     companion object {
-        private const val TAG = "PepperMainActivity"
-        private const val WEBSOCKET_URL = "ws://10.22.25.94:5001/pepper"
+        private const val TAG = "PepperHumanAwareness"
     }
     
     private var qiContext: QiContext? = null
     
-    // WebSocket client
-    private lateinit var webSocketClient: PepperWebSocketClient
+    // Autonomous abilities holder
+    private var autonomousAbilitiesHolder: Holder? = null
     
-    // Camera manager
-    private lateinit var cameraManager: PepperCameraManager
-    
-    // Command dispatcher
-    private var commandDispatcher: CommandDispatcher? = null
-    
-    // Store animations and animate actions
-    private var raiseHandsAnimate: Animate? = null
-    private var danceAnimate: Animate? = null
-    private var currentAnimate: Animate? = null
-    private var isAnimationRunning = false
-    
-    // Human engagement variables
-    private var humanEngagementFuture: Future<Void>? = null
-    private var engageHuman: EngageHuman? = null
-    private var humanCheckTaskRunning = false
-    
-    // Add reconnection timer
-    private var reconnectionTimer: Timer? = null
+    // Human awareness properties
+    private var humanAwareness: HumanAwareness? = null
+    private var currentEngagedHuman: Human? = null
+    private var engageHumanAction: EngageHuman? = null
+    private var engageHumanFuture: Future<Void>? = null
+    private var isEngagementRunning = false
+    private var humanAwarenessInitialized = false
 
     override fun onCreate(savedInstanceState: Bundle?) {
         super.onCreate(savedInstanceState)
         setContentView(R.layout.activity_main)
-        
-        // Initialize WebSocket client
-        webSocketClient = PepperWebSocketClient(WEBSOCKET_URL, this, this)
-        
-        // Load saved WebSocket URL from preferences
-        val sharedPreferences = getSharedPreferences("PepperSettings", Context.MODE_PRIVATE)
-        val savedUrl = sharedPreferences.getString("websocket_url", WEBSOCKET_URL)
-        if (savedUrl != WEBSOCKET_URL) {
-            // Use saved URL if different from default
-            webSocketClient.setServerUrl(savedUrl!!)
-            Log.d(TAG, "Using saved WebSocket URL: $savedUrl")
-        }
-        
-        // Add a button to the layout for settings if needed
-        // For example, you could add a long-press listener to an existing button:
-        responseTextView.setOnLongClickListener {
-            showServerSettings()
-            true
-        }
-        
-        // Initialize camera manager
-        cameraManager = PepperCameraManager(this)
-        
+
         // Register the RobotLifecycleCallbacks
         QiSDK.register(this, this)
-        
-        // Set up button click listeners
-        raiseHandsButton.setOnClickListener {
-            Log.d(TAG, "raiseHandsButton::setOnClickListener called")
-            playAnimation(raiseHandsAnimate, "Raise Hands")
-        }
-        
-        danceButton.setOnClickListener {
-            playAnimation(danceAnimate, "Dance")
-        }
     }
-    
-    private fun showServerSettings() {
-        val input = EditText(this)
-        input.setText(webSocketClient.getServerUrl())
-        
-        AlertDialog.Builder(this)
-            .setTitle("Server Settings")
-            .setMessage("Enter WebSocket Server URL:")
-            .setView(input)
-            .setPositiveButton("Save") { _, _ ->
-                val url = input.text.toString()
-                // Save to preferences
-                getSharedPreferences("PepperSettings", Context.MODE_PRIVATE)
-                    .edit()
-                    .putString("websocket_url", url)
-                    .apply()
-                
-                // Apply the new URL immediately
-                webSocketClient.setServerUrl(url)
-                
-                // Notify user
-                runOnUiThread {
-                    responseTextView.text = "Server URL updated to: $url"
-                }
-            }
-            .setNegativeButton("Cancel", null)
-            .show()
-    }
-    
+
     override fun onDestroy() {
-        // Stop reconnection attempts
-        reconnectionTimer?.cancel()
-        reconnectionTimer = null
-        
-        // Clean up resources
-        webSocketClient.disconnect()
-        cameraManager.release()
-        commandDispatcher?.release()
+        // Release autonomous abilities (re-enable autonomous life) before unregistering
+        releaseAutonomousAbilities()
         
         // Unregister the RobotLifecycleCallbacks
         QiSDK.unregister(this, this)
@@ -153,356 +82,211 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
         // Store the QiContext for later use
         this.qiContext = qiContext
         
-        // Initialize command dispatcher
-        commandDispatcher = CommandDispatcher(qiContext)
-        
-        // Initialize camera
-        cameraManager.initialize(qiContext)
-        
-        // Preload all animations in onRobotFocusGained
-        preloadAnimations(qiContext)
-        
-        // Start the human engagement process
-        startHumanEngagement()
+        // Hold autonomous abilities (disable autonomous life)
+        holdAutonomousAbilities(qiContext)
         
-        // Connect to WebSocket server
-        webSocketClient.connect()
-        
-        runOnUiThread {
-            responseTextView.text = "Robot ready. Connecting to server..."
-        }
+        // Initialize human awareness
+        initializeHumanAwareness(qiContext)
     }
     
     override fun onRobotFocusLost() {
-        // Stop engaging with humans
-        stopHumanEngagement()
-        
-        // Stop camera capture
-        cameraManager.stopCapture()
+        // Stop engagement if running
+        stopEngagement()
         
-        // Clean up resources
-        raiseHandsAnimate?.removeAllOnStartedListeners()
-        danceAnimate?.removeAllOnStartedListeners()
-        currentAnimate = null
-        raiseHandsAnimate = null
-        danceAnimate = null
-        isAnimationRunning = false
-        
-        // Reset QiContext
+        // Release autonomous abilities (re-enable autonomous life)
+        releaseAutonomousAbilities()
+
+        // Reset QiContext and human awareness
+        humanAwareness = null
+        humanAwarenessInitialized = false
         this.qiContext = null
-        commandDispatcher = null
     }
     
     override fun onRobotFocusRefused(reason: String) {
         // Handle focus refused
+        Log.e(TAG, "Robot focus refused: $reason")
         runOnUiThread {
-            "Robot focus refused: $reason".also { responseTextView.text = it }
+            responseTextView.text = "Robot focus refused: $reason"
         }
     }
     
-    //
-    // WebSocket Connection State Listener Implementation
-    //
-    
-    override fun onConnected() {
-        Log.d(TAG, "WebSocket connected")
-//        runOnUiThread {
-//            statusTextView.text = "Connected"
-//            statusTextView.setTextColor(Color.GREEN)
-//        }
-        
-        // Cancel reconnection timer if it's running
-        reconnectionTimer?.cancel()
-        reconnectionTimer = null
-        
-        // Start camera capture when connected
-        cameraManager.startCapture()
-        
-        // Send a timestamp message to measure latency
+    /**
+     * Hold autonomous abilities to disable Pepper's autonomous life
+     */
+    private fun holdAutonomousAbilities(qiContext: QiContext) {
         try {
-            val timestamp = System.currentTimeMillis()
-            val message = JSONObject().apply {
-                put("type", "timestamp")
-                put("timestamp", timestamp)
-            }
-            webSocketClient.sendMessage(message.toString())
-            Log.d(TAG, "Sent timestamp message for latency measurement")
+            // Build the holder for the autonomous abilities
+            val holder = HolderBuilder.with(qiContext)
+                .withAutonomousAbilities(
+                    AutonomousAbilitiesType.BACKGROUND_MOVEMENT,
+                    AutonomousAbilitiesType.BASIC_AWARENESS,
+                    AutonomousAbilitiesType.AUTONOMOUS_BLINKING
+                )
+                .build()
+                
+            // Store the holder
+            autonomousAbilitiesHolder = holder
+            
+            // Hold the abilities asynchronously
+            holder.async().hold().andThenConsume(Qi.onUiThread(Consumer<Void> {
+                Log.i(TAG, "Autonomous abilities held successfully")
+            }))
         } catch (e: Exception) {
-            Log.e(TAG, "Error sending timestamp message", e)
-        }
-    }
-    
-    override fun onDisconnected() {
-        Log.d(TAG, "WebSocket disconnected")
-//        runOnUiThread {
-//            statusTextView.text = "Disconnected"
-//            statusTextView.setTextColor(Color.RED)
-//        }
-        
-        // Pause camera capture when disconnected
-        cameraManager.pauseCapture()
-        
-        // Start reconnection timer if not already running
-        if (reconnectionTimer == null) {
-            reconnectionTimer = Timer()
-            reconnectionTimer?.schedule(object : TimerTask() {
-                override fun run() {
-                    // Try to reconnect
-                    if (webSocketClient != null) {
-                        Log.d(TAG, "Attempting to reconnect to WebSocket server...")
-                        webSocketClient.connect()
-                    }
-                }
-            }, 5000, 5000) // Try every 5 seconds
+            Log.e(TAG, "Error holding autonomous abilities: ${e.message}", e)
         }
     }
     
-    override fun onReconnectFailed() {
-        Log.d(TAG, "WebSocket reconnection failed")
-//        runOnUiThread {
-//            statusTextView.text = "Reconnection Failed"
-//            statusTextView.setTextColor(Color.RED)
-//        }
-        
-        // Stop camera capture when reconnection fails
-        cameraManager.stopCapture()
-    }
-    
-    //
-    // Frame Listener Implementation
-    //
-    
-    override fun onFrameCaptured(imageData: ByteArray, timestamp: Long) {
-        // Send the frame over WebSocket
-        webSocketClient.sendCameraFrame(imageData)
-        
-        // Send a timestamp message for latency tracking
+    /**
+     * Release autonomous abilities to re-enable Pepper's autonomous life
+     */
+    private fun releaseAutonomousAbilities() {
         try {
-            val message = JSONObject().apply {
-                put("type", "frameTimestamp")
-                put("timestamp", timestamp)
-            }
-            webSocketClient.sendMessage(message.toString())
+            // Get the holder
+            val holder = autonomousAbilitiesHolder ?: return
+            
+            // Release the holder asynchronously
+            holder.async().release().andThenConsume(Qi.onUiThread(Consumer<Void> {
+                Log.i(TAG, "Autonomous abilities released successfully")
+                autonomousAbilitiesHolder = null
+            }))
         } catch (e: Exception) {
-            Log.e(TAG, "Error sending timestamp", e)
+            Log.e(TAG, "Error releasing autonomous abilities: ${e.message}", e)
         }
     }
     
-    //
-    // Command Listener Implementation
-    //
-    
-    override fun onCommandReceived(command: JSONObject) {
-        Log.d(TAG, "Command received in MainActivity: $command")
-        
-        // Dispatch the command to the CommandDispatcher
-        commandDispatcher?.dispatch(command)
-    }
-    
-    //
-    // Animation Methods
-    //
-    
-    private fun preloadAnimations(qiContext: QiContext) {
+    /**
+     * Initialize human awareness and set up human detection
+     */
+    private fun initializeHumanAwareness(qiContext: QiContext) {
         try {
-            // Load the raise hands animation
-            val raiseHandsAnimation = AnimationBuilder.with(qiContext)
-                .withResources(R.raw.raise_both_hands_b001)
-                .build()
-                
-            // Create the animate action for raise hands
-            raiseHandsAnimate = AnimateBuilder.with(qiContext)
-                .withAnimation(raiseHandsAnimation)
-                .build()
-                
-            // Load the dance animation
-            val danceAnimation = AnimationBuilder.with(qiContext)
-                .withResources(R.raw.dance_b001)
-                .build()
-                
-            // Create the animate action for dance
-            danceAnimate = AnimateBuilder.with(qiContext)
-                .withAnimation(danceAnimation)
-                .build()
-                
-            // Enable buttons when animations are loaded
-            runOnUiThread {
-                raiseHandsButton.isEnabled = true
-                danceButton.isEnabled = true
+            // Get the HumanAwareness service
+            humanAwareness = qiContext.humanAwareness
+            
+            // Set up listeners for humans
+            humanAwareness?.addOnHumansAroundChangedListener { humans ->
+                processHumansAround(humans)
             }
             
+            humanAwarenessInitialized = true
+            Log.i(TAG, "Human awareness initialized successfully")
+            updateStatus("Human awareness active - looking for humans")
+            
         } catch (e: Exception) {
-            Log.e(TAG, "Error loading animations: ${e.message}")
-            runOnUiThread {
-                responseTextView.text = "Error loading animations: ${e.message}"
-            }
+            Log.e(TAG, "Error initializing human awareness: ${e.message}", e)
+            updateStatus("Failed to initialize human awareness")
         }
     }
     
-    private fun playAnimation(animate: Animate?, animationName: String) {
-        Log.d(TAG, "currently in playAnimation(Animate?, String)")
-
-        // Check if an animation is already running
-        if (isAnimationRunning) {
-            responseTextView.text = "Animation already running, please wait..."
+    /**
+     * Process the list of humans detected around the robot
+     */
+    private fun processHumansAround(humans: List<Human>) {
+        // Debug output showing number of humans detected
+        Log.d(TAG, "Humans detected: ${humans.size}")
+        
+        if (humans.isEmpty()) {
             return
-        }
-        
-        if (animate == null) {
-            responseTextView.text = "Animation not loaded. Try again later."
-            return
-        }
-        
-        // Show progress indicator
-        progressBar.visibility = View.VISIBLE
-        responseTextView.text = "Starting $animationName animation..."
-        
-        // Store current animate action
-        currentAnimate = animate
-        
-        // Mark animation as running
-        isAnimationRunning = true
-        
-        // Run the animation asynchronously
-        animate.async().run().thenConsume { future ->
-            isAnimationRunning = false
-            runOnUiThread {
-                if (future.isSuccess) {
-                    responseTextView.text = "$animationName animation completed"
-                } else if (future.hasError()) {
-                    val error = future.error
-                    Log.e(TAG, "Error during animation: ${error.message}")
-                    responseTextView.text = "Animation error: ${error.message}"
-                }
-                progressBar.visibility = View.GONE
-                
-                // Remove the listener after animation completes
-                animate.removeAllOnStartedListeners()
-            }
+        } else {
+            // Found at least one engaged human
+            engageWithHuman(humans.first())
         }
     }
     
-    //
-    // Human Engagement Methods
-    //
-    
-    private fun startHumanEngagement() {
-        qiContext?.let { context ->
-            humanCheckTaskRunning = true
+    /**
+     * Engage with a specific human
+     */
+    private fun engageWithHuman(human: Human) {
+        val ctx = qiContext ?: return
+        
+        try {
+            // Stop any existing engagement
+            stopEngagement()
             
-            // Start a periodic task to find and engage with humans
-            Thread {
-                try {
-                    while (humanCheckTaskRunning && qiContext != null) {
-                        findMostEngagedHuman()
-                        // Check for engaged humans every 3 seconds
-                        TimeUnit.SECONDS.sleep(3)
-                    }
-                } catch (e: InterruptedException) {
-                    Log.d(TAG, "Human engagement check interrupted")
-                } catch (e: Exception) {
-                    Log.e(TAG, "Error in human engagement check: ${e.message}")
-                }
-            }.start()
+            // Store the human we're engaging with
+            currentEngagedHuman = human
+            
+            // Debug additional information about the human
+            Log.d(TAG, "Engaging with human: attention=${human.attention}, excitement=${human.emotion?.excitement}")
             
-            runOnUiThread {
-                responseTextView.text = "Looking for humans to engage with..."
+            // Build the engage human action
+            engageHumanAction = EngageHumanBuilder.with(ctx)
+                .withHuman(human)
+                .build()
+            
+            // Run the engagement
+            Log.i(TAG, "Starting engagement with human")
+            isEngagementRunning = true
+            
+            // Run engagement asynchronously
+            engageHumanFuture = engageHumanAction?.async()?.run()
+            
+            // Add a listener for when engagement completes or fails
+            engageHumanFuture?.thenConsume { future ->
+                if (future.isSuccess) {
+                    Log.i(TAG, "Engagement completed successfully")
+                } else if (future.isCancelled) {
+                    Log.i(TAG, "Engagement was cancelled")
+                } else {
+                    Log.e(TAG, "Engagement error: ${future.error.message}")
+                }
+                isEngagementRunning = false
+                currentEngagedHuman = null
             }
+            
+        } catch (e: Exception) {
+            Log.e(TAG, "Error engaging with human: ${e.message}", e)
+            isEngagementRunning = false
+            currentEngagedHuman = null
         }
     }
     
-    private fun stopHumanEngagement() {
-        // Stop the periodic human check
-        humanCheckTaskRunning = false
-        
-        // Cancel any active engagement
-        humanEngagementFuture?.requestCancellation()
-        humanEngagementFuture = null
-        
-        // Clean up the engage human action
-        engageHuman = null
-    }
-    
-    private fun findMostEngagedHuman() {
-        qiContext?.let { context ->
+    /**
+     * Stop the current engagement action
+     */
+    private fun stopEngagement() {
+        if (isEngagementRunning) {
             try {
-                // Get the human awareness service
-                val humanAwareness = context.humanAwareness
-                
-                // Get humans around the robot
-                val humansAround = humanAwareness.humansAround
-                
-                if (humansAround.isEmpty()) {
-                    Log.d(TAG, "No humans detected")
-                    runOnUiThread {
-                        responseTextView.text = "No humans detected"
-                    }
-                    return
-                }
+                // Cancel the future
+                engageHumanFuture?.requestCancellation()
+                engageHumanFuture = null
                 
-                Log.d(TAG, "Found ${humansAround.size} humans around")
-                
-                // Find the human with the highest engagement intention
-                val mostEngagedHuman = findHumanWithHighestEngagement(humansAround)
-                
-                if (mostEngagedHuman != null) {
-                    engageWithHuman(mostEngagedHuman)
-                } else {
-                    Log.d(TAG, "No humans showing engagement")
-                    runOnUiThread {
-                        responseTextView.text = "No humans engaging"
-                    }
-                }
+                // Reset engagement state
+                engageHumanAction = null
+                currentEngagedHuman = null
+                isEngagementRunning = false
                 
+                Log.i(TAG, "Engagement stopped")
             } catch (e: Exception) {
-                Log.e(TAG, "Error finding humans: ${e.message}")
+                Log.e(TAG, "Error stopping engagement: ${e.message}", e)
             }
         }
     }
-    
-    private fun findHumanWithHighestEngagement(humans: List<Human>): Human? {
-        // Find humans that are engaged or want to engage
-        val engagedHumans = humans.filter { 
-            it.engagementIntention == EngagementIntentionState.INTERESTED
-        }
-        
-        // If we have engaged humans, return the first one
-        if (engagedHumans.isNotEmpty()) {
-            Log.d(TAG, "Found ${engagedHumans.size} engaged humans")
-            return engagedHumans.first()
-        }
-        
-        // Otherwise, return the first human that at least has attention toward the robot
-        return humans.firstOrNull { 
-            it.attention.toString() == "LOOKING_AT" 
+
+    /**
+     * Update the status text on the UI thread
+     */
+    private fun updateStatus(message: String) {
+        Log.i(TAG, "Status update: $message")
+        runOnUiThread {
+            responseTextView.text = message
         }
     }
     
-    private fun engageWithHuman(human: Human) {
-        qiContext?.let { context ->
-            // Cancel any existing engagement
-            humanEngagementFuture?.requestCancellation()
-            
-            try {
-                // Create a new engage human action
-                engageHuman = EngageHumanBuilder.with(context)
-                    .withHuman(human)
-                    .build()
-                
-                // Start engaging with the human
-                humanEngagementFuture = engageHuman?.async()?.run()
-                
-                Log.d(TAG, "Engaging with human: ${human.attention}, ${human.engagementIntention}")
+    /**
+     * Say text using the robot's speech synthesis
+     */
+    private fun sayText(text: String) {
+        val ctx = qiContext ?: return
+        
+        try {
+            // Build and run the say action
+            val say = SayBuilder.with(ctx)
+                .withText(text)
+                .build()
                 
-                runOnUiThread {
-                    responseTextView.text = "Engaging with human"
-                }
-            } catch (e: Exception) {
-                Log.e(TAG, "Error engaging with human: ${e.message}")
-                runOnUiThread {
-                    responseTextView.text = "Error engaging: ${e.message}"
-                }
-            }
+            say.run()
+        } catch (e: Exception) {
+            Log.e(TAG, "Error saying text: ${e.message}", e)
         }
     }
 }
\ No newline at end of file
diff --git a/app/src/main/java/com/example/peppertest/camera/PepperCameraManager.kt b/app/src/main/java/com/example/peppertest/camera/PepperCameraManager.kt
index 62e2389..86dfdd5 100644
--- a/app/src/main/java/com/example/peppertest/camera/PepperCameraManager.kt
+++ b/app/src/main/java/com/example/peppertest/camera/PepperCameraManager.kt
@@ -22,7 +22,7 @@ import java.util.concurrent.atomic.AtomicBoolean
 class PepperCameraManager(private val frameListener: FrameListener) {
     companion object {
         private const val TAG = "PepperCameraManager"
-        private const val TARGET_FPS = 10
+        private const val TARGET_FPS = 30
         private const val FRAME_INTERVAL_MS = 1000L / TARGET_FPS
     }
     
diff --git a/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt b/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt
index 612226a..5522ed9 100644
--- a/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt
+++ b/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt
@@ -168,16 +168,31 @@ class PepperWebSocketClient(
                     Log.d(TAG, "Received message: $text")
                     val json = JSONObject(text)
                     
-                    // Check if this is a command message
-                    if (json.has("type") && json.getString("type") == "command") {
-                        Log.d(TAG, "Received command: ${json.getString("action")}")
-                        commandListener.onCommandReceived(json)
-                    } else if (json.has("type") && json.getString("type") == "pong") {
-                        // Handle pong response
-                        Log.d(TAG, "Received pong response")
-                        lastPongTime = System.currentTimeMillis()
+                    // Check message type
+                    if (json.has("type")) {
+                        val type = json.getString("type")
+                        
+                        // Handle different message types
+                        when (type) {
+                            "command" -> {
+                                Log.d(TAG, "Received command: ${json.getString("action")}")
+                                commandListener.onCommandReceived(json)
+                            }
+                            "face_detection" -> {
+                                Log.d(TAG, "Received face detection command: ${json.optString("action", "")}")
+                                commandListener.onCommandReceived(json)
+                            }
+                            "pong" -> {
+                                // Handle pong response
+                                Log.d(TAG, "Received pong response")
+                                lastPongTime = System.currentTimeMillis()
+                            }
+                            else -> {
+                                Log.d(TAG, "Received unknown message type: $type")
+                            }
+                        }
                     } else {
-                        Log.d(TAG, "Received unknown message type: ${json.optString("type", "no type")}")
+                        Log.d(TAG, "Received message without type")
                     }
                 } catch (e: Exception) {
                     Log.e(TAG, "Error parsing message", e)
diff --git a/pepper_vision/README.md b/pepper_vision/README.md
index 7255a1e..67e1faf 100644
--- a/pepper_vision/README.md
+++ b/pepper_vision/README.md
@@ -1,6 +1,6 @@
 # Pepper Vision System
 
-A system for streaming camera feed from a Pepper robot to Python clients and web browsers.
+A system for streaming camera feed from a Pepper robot to Python clients and web browsers with face detection capabilities.
 
 ## System Overview
 
@@ -10,19 +10,35 @@ This system consists of several components:
    - Acts as the central hub that receives camera frames from Pepper
    - Forwards frames to connected clients
    - Handles bidirectional communication for commands
+   - Processes and forwards face detection data from Pepper
 
-2. **Python Client** (webcam_client.py):
-   - Connects to the WebSocket server
-   - Displays the camera feed from Pepper
-   - Can send commands to Pepper
+2. **Android App** (Pepper Robot):
+   - Captures camera frames from Pepper's camera
+   - Performs face detection using Pepper's built-in SDK
+   - Sends both frames and face detection data to the server
+   - Executes commands received from the server
 
 3. **Web Interface** (static/index.html):
    - Allows viewing the camera feed in a browser
    - Provides a GUI for sending commands to Pepper
+   - Displays face detection results with visual overlays
+   - Controls for enabling/disabling face detection
 
 4. **Static File Server** (static_server.py):
    - Serves the web interface
 
+## Features
+
+### Real-time Face Detection
+
+The system includes real-time face detection functionality:
+
+- **Pepper SDK processing**: Face detection runs on the Pepper robot using its built-in SDK
+- **Client-side visualization**: Detected faces are highlighted on the video feed
+- **Additional face data**: Shows age, gender, attention state, and smile state when available
+- **Face tracking**: Each detected face is assigned a consistent ID for tracking
+- **Toggle on/off**: Enable or disable face detection as needed
+
 ## Setup Instructions
 
 ### 1. Install Dependencies
@@ -85,6 +101,7 @@ To view the camera feed in a web browser:
 2. Navigate to: `http://YOUR_SERVER_IP:8000`
 3. Click "Connect" to establish the WebSocket connection
 4. You should see the Pepper camera feed and be able to send commands
+5. Use the "Enable Face Detection" button to activate face detection
 
 ### 5. Use the Python Client
 
@@ -110,22 +127,32 @@ python webcam_client.py ws://YOUR_SERVER_IP:5001
 ## System Architecture
 
 ```
-┌────────────────┐   WebSocket   ┌───────────────┐   WebSocket   ┌───────────────┐
-│  Pepper Robot  │ ──────────► │ WebSocket Server │ ──────────► │ Python Client  │
-└────────────────┘   (frames)   └───────────────┘   (frames)    └───────────────┘
-                                      ▲  │
-                                      │  │ WebSocket
-                                      │  ▼
-                                ┌───────────────┐
-                                │  Web Browser  │
-                                └───────────────┘
+┌────────────────┐   WebSocket   ┌───────────────────────┐   WebSocket   ┌───────────────┐
+│  Pepper Robot  │ ──────────► │ WebSocket Server      │ ──────────► │ Python Client  │
+│  (Face Detection)│   (frames +   └───────────────────────┘   (frames)    └───────────────┘
+└────────────────┘   face data)          ▲  │
+                                         │  │ WebSocket
+                                         │  ▼
+                                   ┌───────────────────────────┐
+                                   │  Web Browser + Face Display │
+                                   └───────────────────────────┘
 ```
 
+## Face Detection Details
+
+The face detection system uses Pepper's built-in capabilities:
+
+- **HumanAwareness API**: Uses Pepper's human awareness feature to detect and track people
+- **3D to 2D Conversion**: Converts Pepper's 3D coordinates to 2D screen coordinates
+- **Additional Attributes**: Extracts age, gender, smile state, and attention state from Pepper's SDK
+- **WebSocket Protocol**: Sends face data as JSON alongside binary frame data
+
 ## Customizing
 
 - **Frame Rate**: Adjust the frame rate in the Python client by changing the `asyncio.sleep()` value.
 - **Image Quality**: Adjust JPEG compression in the Pepper app to balance between quality and bandwidth.
 - **Commands**: Add new command types and handlers as needed.
+- **Face Detection**: Adjust face detection interval by modifying the `FACE_DETECTION_INTERVAL_MS` constant in MainActivity.kt.
 
 ## Troubleshooting
 
@@ -133,6 +160,7 @@ python webcam_client.py ws://YOUR_SERVER_IP:5001
 - Ensure no firewalls are blocking the WebSocket connections on port 5001.
 - Check the log files in the `logs` directory for error messages.
 - If the Pepper robot can't connect, verify the correct IP address is being used.
+- If face detection doesn't work, check that the QiSDK is properly initialized and the robot has focus.
 
 ## License
 
diff --git a/pepper_vision/server.py b/pepper_vision/server.py
index acb2a4a..fb7412d 100644
--- a/pepper_vision/server.py
+++ b/pepper_vision/server.py
@@ -6,6 +6,12 @@ import os
 from datetime import datetime
 import sys
 import websockets
+import cv2
+import numpy as np
+import uuid
+
+# Global settings
+FACE_DETECTION_ENABLED = False
 
 # Check websockets version and add version-specific imports
 WEBSOCKETS_VERSION = websockets.__version__
@@ -47,6 +53,7 @@ class PepperServer:
         self.clients = set()
         self.pepper_connection = None
         self.latest_frame = None
+        self.latest_face_data = None
     
     def get_ip_address(self):
         """Get the local IP address for network connectivity"""
@@ -71,7 +78,7 @@ class PepperServer:
         
         try:
             async for message in websocket:
-                # If message is from Pepper, it's a camera frame
+                # If message is from Pepper, it's a camera frame or JSON data
                 if isinstance(message, bytes):
                     # Store the latest frame
                     self.latest_frame = message
@@ -82,11 +89,36 @@ class PepperServer:
                             *[client.send(message) for client in self.clients],
                             return_exceptions=True
                         )
-                # Handle JSON messages from Pepper
+                        
+                        # If we have face data from Pepper, send it after the frame
+                        if self.latest_face_data and FACE_DETECTION_ENABLED:
+                            await asyncio.gather(
+                                *[client.send(json.dumps(self.latest_face_data)) for client in self.clients],
+                                return_exceptions=True
+                            )
+                
+                # Handle JSON messages from Pepper (commands and face detection data)
                 elif isinstance(message, str):
                     try:
                         data = json.loads(message)
                         logger.debug(f"Received message from Pepper: {data}")
+                        
+                        # If message is face detection data
+                        if data.get("type") == "face_detection" and data.get("source") == "pepper_sdk":
+                            logger.info(f"Received face detection data from Pepper: {len(data.get('faces', []))} faces")
+                            
+                            # Store the latest face data
+                            self.latest_face_data = data
+                            
+                            # Forward face data to clients if face detection is enabled
+                            if self.clients and FACE_DETECTION_ENABLED:
+                                await asyncio.gather(
+                                    *[client.send(message) for client in self.clients],
+                                    return_exceptions=True
+                                )
+                        # Other types of messages
+                        else:
+                            logger.debug(f"Received other message type: {data.get('type')}")
                     except json.JSONDecodeError:
                         logger.warning(f"Received invalid JSON from Pepper")
         
@@ -104,6 +136,8 @@ class PepperServer:
     
     async def handle_client(self, websocket):
         """Handle WebSocket connections from regular clients"""
+        global FACE_DETECTION_ENABLED
+        
         client_id = f"client-{id(websocket)}"
         logger.info(f"Webcam client connected: {client_id}")
         
@@ -118,8 +152,36 @@ class PepperServer:
                         command = json.loads(message)
                         logger.info(f"Received command from client {client_id}: {command}")
                         
-                        # Forward command to Pepper if connected
-                        if self.pepper_connection:
+                        # Handle face detection toggle command
+                        if command.get("type") == "face_detection":
+                            action = command.get("action")
+                            if action == "enable":
+                                FACE_DETECTION_ENABLED = True
+                                logger.info("Face detection enabled")
+                                # Send confirmation to the client
+                                await websocket.send(json.dumps({"type": "face_detection", "status": "enabled"}))
+                                
+                                # Forward the face detection enable command to Pepper
+                                if self.pepper_connection:
+                                    logger.info("Forwarding face detection enable command to Pepper")
+                                    await self.pepper_connection.send(json.dumps(command))
+                                
+                                # If we have face data and just enabled detection, send it
+                                if self.latest_face_data:
+                                    await websocket.send(json.dumps(self.latest_face_data))
+                            elif action == "disable":
+                                FACE_DETECTION_ENABLED = False
+                                logger.info("Face detection disabled")
+                                # Send confirmation to the client
+                                await websocket.send(json.dumps({"type": "face_detection", "status": "disabled"}))
+                                
+                                # Forward the face detection disable command to Pepper
+                                if self.pepper_connection:
+                                    logger.info("Forwarding face detection disable command to Pepper")
+                                    await self.pepper_connection.send(json.dumps(command))
+                        
+                        # Forward other commands to Pepper if connected
+                        elif self.pepper_connection:
                             await self.pepper_connection.send(json.dumps(command))
                         else:
                             logger.warning("Cannot forward command: Pepper not connected")
diff --git a/pepper_vision/static/index.html b/pepper_vision/static/index.html
index 13936e4..461a9ea 100644
--- a/pepper_vision/static/index.html
+++ b/pepper_vision/static/index.html
@@ -27,12 +27,19 @@
             width: 100%;
             text-align: center;
             margin: 20px 0;
+            position: relative;
         }
         #video-feed {
             max-width: 100%;
             border: 1px solid #ddd;
             background-color: #eee;
         }
+        #face-canvas {
+            position: absolute;
+            top: 0;
+            left: 0;
+            pointer-events: none;
+        }
         .controls {
             margin-top: 20px;
             display: flex;
@@ -73,6 +80,22 @@
         button:hover {
             background-color: #45a049;
         }
+        button.toggle-off {
+            background-color: #f44336;
+        }
+        button.toggle-off:hover {
+            background-color: #d32f2f;
+        }
+        .face-info {
+            position: absolute;
+            top: 0;
+            left: 0;
+            background-color: rgba(0, 0, 0, 0.5);
+            color: white;
+            font-size: 12px;
+            padding: 2px 4px;
+            border-radius: 3px;
+        }
         input[type="text"] {
             padding: 10px;
             width: 100%;
@@ -87,6 +110,27 @@
         .server-input input {
             flex-grow: 1;
         }
+        .face-controls {
+            margin-top: 20px;
+            padding: 15px;
+            border: 1px solid #ddd;
+            border-radius: 4px;
+            background-color: #f9f9f9;
+        }
+        .face-stats {
+            margin-top: 10px;
+            font-size: 14px;
+            color: #666;
+        }
+        .control-group {
+            margin-top: 10px;
+            display: flex;
+            gap: 10px;
+            align-items: center;
+        }
+        .control-label {
+            min-width: 120px;
+        }
     </style>
 </head>
 <body>
@@ -102,6 +146,16 @@
         
         <div class="video-container">
             <img id="video-feed" width="640" height="480" alt="Pepper Camera Feed">
+            <canvas id="face-canvas" width="640" height="480"></canvas>
+        </div>
+        
+        <div class="face-controls">
+            <h3>Face Detection</h3>
+            <button id="face-detection-btn" class="toggle-off">Enable Face Detection</button>
+            <div class="face-stats">
+                <div>Detected faces: <span id="face-count">0</span></div>
+                <div>Data source: <span id="face-source">Pepper SDK</span></div>
+            </div>
         </div>
         
         <div class="controls">
@@ -125,9 +179,13 @@
     <script>
         let socket = null;
         let isConnected = false;
+        let faceDetectionEnabled = false;
+        let currentFaces = [];
         
         // DOM elements
         const videoFeed = document.getElementById('video-feed');
+        const faceCanvas = document.getElementById('face-canvas');
+        const faceContext = faceCanvas.getContext('2d');
         const statusBar = document.getElementById('status-bar');
         const serverUrlInput = document.getElementById('server-url');
         const connectBtn = document.getElementById('connect-btn');
@@ -139,6 +197,9 @@
         const backwardBtn = document.getElementById('backward-btn');
         const turnLeftBtn = document.getElementById('turn-left-btn');
         const turnRightBtn = document.getElementById('turn-right-btn');
+        const faceDetectionBtn = document.getElementById('face-detection-btn');
+        const faceCountEl = document.getElementById('face-count');
+        const faceSourceEl = document.getElementById('face-source');
         
         // Auto-detect server IP (same subnet as client)
         function autoDetectServerUrl() {
@@ -155,6 +216,18 @@
             const autoUrl = autoDetectServerUrl();
             serverUrlInput.value = autoUrl;
             
+            // Connect canvas size to video size
+            function updateCanvasSize() {
+                const rect = videoFeed.getBoundingClientRect();
+                faceCanvas.width = rect.width;
+                faceCanvas.height = rect.height;
+            }
+            
+            // Update canvas size on window resize
+            window.addEventListener('resize', updateCanvasSize);
+            videoFeed.addEventListener('load', updateCanvasSize);
+            
+            // Set up event listeners
             connectBtn.addEventListener('click', connectToServer);
             sayBtn.addEventListener('click', sayCommand);
             waveBtn.addEventListener('click', () => animateCommand('wave'));
@@ -163,6 +236,124 @@
             backwardBtn.addEventListener('click', () => moveCommand(-1, 0, 0));
             turnLeftBtn.addEventListener('click', () => moveCommand(0, 0, 1));
             turnRightBtn.addEventListener('click', () => moveCommand(0, 0, -1));
+            
+            // Face detection controls
+            faceDetectionBtn.addEventListener('click', toggleFaceDetection);
+        }
+        
+        // Toggle face detection
+        function toggleFaceDetection() {
+            if (!isConnected) {
+                alert('Please connect to the server first');
+                return;
+            }
+            
+            faceDetectionEnabled = !faceDetectionEnabled;
+            
+            if (faceDetectionEnabled) {
+                enableFaceDetection();
+                faceDetectionBtn.textContent = 'Disable Face Detection';
+                faceDetectionBtn.classList.remove('toggle-off');
+            } else {
+                disableFaceDetection();
+                faceDetectionBtn.textContent = 'Enable Face Detection';
+                faceDetectionBtn.classList.add('toggle-off');
+                clearFaceCanvas();
+            }
+        }
+        
+        // Enable face detection on the server
+        function enableFaceDetection() {
+            const command = {
+                type: 'face_detection',
+                action: 'enable'
+            };
+            
+            socket.send(JSON.stringify(command));
+        }
+        
+        // Disable face detection on the server
+        function disableFaceDetection() {
+            const command = {
+                type: 'face_detection',
+                action: 'disable'
+            };
+            
+            socket.send(JSON.stringify(command));
+        }
+        
+        // Draw face rectangles on the canvas
+        function drawFaces(faces) {
+            if (!faces || faces.length === 0) {
+                return;
+            }
+            
+            // Get the scale to map face coordinates to canvas
+            const scaleX = faceCanvas.width / 640;
+            const scaleY = faceCanvas.height / 480;
+            
+            // Clear previous drawings
+            clearFaceCanvas();
+            
+            // Update face count
+            faceCountEl.textContent = faces.length;
+            
+            // Draw each face
+            faces.forEach(face => {
+                const x = face.x * scaleX;
+                const y = face.y * scaleY;
+                const width = face.width * scaleX;
+                const height = face.height * scaleY;
+                
+                // Draw rectangle
+                faceContext.lineWidth = 3;
+                faceContext.strokeStyle = 'rgba(0, 255, 0, 0.8)';
+                faceContext.strokeRect(x, y, width, height);
+                
+                // Draw face ID
+                faceContext.font = '14px Arial';
+                faceContext.fillStyle = 'rgba(0, 255, 0, 0.9)';
+                faceContext.fillText(`ID: ${face.id}`, x, y - 5);
+                
+                // Draw additional attributes if available
+                if (face.attributes) {
+                    let attributeText = '';
+                    let yOffset = y + height + 15;
+                    
+                    // Add age if available
+                    if (face.attributes.age) {
+                        attributeText = `Age: ${face.attributes.age}`;
+                        faceContext.fillText(attributeText, x, yOffset);
+                        yOffset += 15;
+                    }
+                    
+                    // Add gender if available
+                    if (face.attributes.gender) {
+                        attributeText = `Gender: ${face.attributes.gender}`;
+                        faceContext.fillText(attributeText, x, yOffset);
+                        yOffset += 15;
+                    }
+                    
+                    // Add smile state if available
+                    if (face.attributes.smileState) {
+                        attributeText = `Smile: ${face.attributes.smileState}`;
+                        faceContext.fillText(attributeText, x, yOffset);
+                        yOffset += 15;
+                    }
+                    
+                    // Add attention state if available
+                    if (face.attributes.attentionState) {
+                        attributeText = `Attention: ${face.attributes.attentionState}`;
+                        faceContext.fillText(attributeText, x, yOffset);
+                    }
+                }
+            });
+        }
+        
+        // Clear the face detection canvas
+        function clearFaceCanvas() {
+            faceContext.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
+            faceCountEl.textContent = '0';
         }
         
         // Connect to WebSocket server
@@ -201,6 +392,21 @@
                         try {
                             const message = JSON.parse(event.data);
                             console.log('Received message:', message);
+                            
+                            // Handle face detection data
+                            if (message.type === 'face_detection') {
+                                if (message.faces) {
+                                    currentFaces = message.faces;
+                                    drawFaces(message.faces);
+                                    
+                                    // Update source info
+                                    if (message.source) {
+                                        faceSourceEl.textContent = message.source;
+                                    }
+                                } else if (message.status) {
+                                    console.log(`Face detection ${message.status}`);
+                                }
+                            }
                         } catch (error) {
                             console.error('Invalid JSON message:', event.data);
                         }
@@ -209,9 +415,13 @@
                 
                 socket.onclose = () => {
                     isConnected = false;
+                    faceDetectionEnabled = false;
                     statusBar.textContent = 'Disconnected from server';
                     statusBar.className = 'status disconnected';
                     connectBtn.textContent = 'Connect';
+                    faceDetectionBtn.textContent = 'Enable Face Detection';
+                    faceDetectionBtn.classList.add('toggle-off');
+                    clearFaceCanvas();
                 };
                 
                 socket.onerror = (error) => {
