diff --git a/app/src/main/java/com/example/peppertest/MainActivity.kt b/app/src/main/java/com/example/peppertest/MainActivity.kt
index c0e244d..e0046f8 100644
--- a/app/src/main/java/com/example/peppertest/MainActivity.kt
+++ b/app/src/main/java/com/example/peppertest/MainActivity.kt
@@ -7,6 +7,7 @@ import android.os.Bundle
 import android.util.Log
 import android.view.View
 import android.widget.EditText
+import android.widget.Switch
 import androidx.appcompat.app.AlertDialog
 import com.aldebaran.qi.Consumer
 import com.aldebaran.qi.Future
@@ -42,13 +43,18 @@ import java.util.Timer
 import java.util.TimerTask
 import java.util.concurrent.TimeUnit
 import java.util.concurrent.atomic.AtomicBoolean
+import com.aldebaran.qi.sdk.`object`.conversation.ConversationStatus
+import com.aldebaran.qi.sdk.`object`.conversation.Listen
+import com.aldebaran.qi.sdk.`object`.conversation.ListenResult
 
 class MainActivity : RobotActivity(), RobotLifecycleCallbacks, 
                      PepperWebSocketClient.CommandListener,
                      PepperWebSocketClient.ConnectionStateListener {
     companion object {
         private const val TAG = "PepperHumanAwareness"
-        private const val DEFAULT_WEBSOCKET_URL = "ws://10.22.25.94:5001/pepper" // Default WebSocket URL
+        private const val DEFAULT_WEBSOCKET_URL = "ws://10.22.25.94:5003/pepper" // Default WebSocket URL
+        private const val SPEAKER_SWITCH_DELAY_MS = 1000 // Delay before switching speakers
+        private const val SPEECH_TIMEOUT_MS = 5000 // Time after speech stops to return to soft engagement
     }
     
     private var qiContext: QiContext? = null
@@ -64,10 +70,22 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
     private var isEngagementRunning = false
     private var humanAwarenessInitialized = false
     
+    // Conversation tracking properties
+    private var conversationStatus: ConversationStatus? = null
+    private var lastSpeakingHuman: Human? = null
+    private var lastSpeechTimestamp: Long = 0
+    private var isSpeechDetectionActive = false
+    private var currentEngagementPolicy: EngagementPolicy = EngagementPolicy.STRICT
+    
     // WebSocket client
     private var webSocketClient: PepperWebSocketClient? = null
     private var websocketServerUrl = DEFAULT_WEBSOCKET_URL
     private var isSpeaking = AtomicBoolean(false)
+    private var isWebSocketEnabled = false  // Disabled by default
+
+    // Add timer for speech detection timeout
+    private var speechTimeoutTimer: Timer? = null
+    private var speechTimeoutTask: TimerTask? = null
 
     override fun onCreate(savedInstanceState: Bundle?) {
         super.onCreate(savedInstanceState)
@@ -79,6 +97,9 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
         // Initialize WebSocket URL from preferences if available
         websocketServerUrl = getPreferences(Context.MODE_PRIVATE)
             .getString("websocket_url", DEFAULT_WEBSOCKET_URL) ?: DEFAULT_WEBSOCKET_URL
+        
+        // Setup WebSocket toggle
+        setupWebSocketToggle()
     }
 
     override fun onDestroy() {
@@ -103,14 +124,22 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
         // Initialize human awareness
         initializeHumanAwareness(qiContext)
         
-        // Connect to WebSocket server
-        connectWebSocket()
+        // Only connect to WebSocket if enabled
+        if (isWebSocketEnabled) {
+            connectWebSocket()
+        } else {
+            Log.i(TAG, "WebSocket connection disabled")
+            updateStatus("WebSocket connection disabled")
+        }
     }
     
     override fun onRobotFocusLost() {
         // Stop engagement if running
         stopEngagement()
         
+        // Clean up conversation tracking
+        cleanupConversationTracking()
+        
         // Disconnect WebSocket if connected
         disconnectWebSocket()
         
@@ -200,6 +229,33 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
                         }
                     }
                 }
+                "speech" -> {
+                    // Handle speech messages (direct from server)
+                    if (command.has("action")) {
+                        val action = command.getString("action")
+                        
+                        when (action) {
+                            "say" -> {
+                                // Handle text-to-speech command
+                                if (command.has("text")) {
+                                    val text = command.getString("text")
+                                    if (text.isNotEmpty()) {
+                                        Log.i(TAG, "Speaking text from speech message: $text")
+                                        runOnUiThread {
+                                            updateStatus("Speaking: $text")
+                                        }
+                                        
+                                        // Use animation for server-generated responses to make them more engaging
+                                        sayTextWithAnimation(text)
+                                    }
+                                }
+                            }
+                            else -> {
+                                Log.d(TAG, "Unhandled speech action: $action")
+                            }
+                        }
+                    }
+                }
                 "speak" -> {
                     // Original format for backward compatibility
                     if (command.has("text")) {
@@ -372,12 +428,185 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
             Log.i(TAG, "Human awareness initialized successfully")
             updateStatus("Human awareness active - looking for humans")
             
+            // Initialize conversation tracking
+            initializeConversationTracking(qiContext)
+            
         } catch (e: Exception) {
             Log.e(TAG, "Error initializing human awareness: ${e.message}", e)
             updateStatus("Failed to initialize human awareness")
         }
     }
     
+    /**
+     * Initialize conversation tracking to detect who is speaking
+     */
+    private fun initializeConversationTracking(qiContext: QiContext) {
+        try {
+            // Get conversation status for this robot context
+            conversationStatus = qiContext.conversation.status(qiContext.robotContext)
+            
+            // Add listener for when speech is detected
+            conversationStatus?.addOnHeardListener { phrase ->
+                // Speech detected - identify which human is speaking
+                onSpeechDetected(phrase)
+            }
+            
+            // Add listener for when robot is speaking
+            conversationStatus?.addOnSayingChangedListener { phrase ->
+                // When robot is speaking, we should maintain focus on the human we're talking to
+                if (phrase.text.isNotEmpty()) {
+                    // Robot is speaking, maintain current focus
+                    Log.d(TAG, "Robot is speaking: ${phrase.text}")
+                } else {
+                    // Robot finished speaking
+                    Log.d(TAG, "Robot finished speaking")
+                }
+            }
+            
+            isSpeechDetectionActive = true
+            Log.i(TAG, "Conversation tracking initialized successfully")
+            
+        } catch (e: Exception) {
+            Log.e(TAG, "Error initializing conversation tracking: ${e.message}", e)
+            isSpeechDetectionActive = false
+        }
+    }
+    
+    /**
+     * Process speech detected from a human
+     */
+    private fun onSpeechDetected(phrase: Phrase) {
+        val currentTimestamp = System.currentTimeMillis()
+        Log.d(TAG, "Speech detected: ${phrase.text}")
+        
+        // Get current list of humans around
+        val humans = humanAwareness?.humansAround ?: return
+        if (humans.isEmpty()) {
+            return
+        }
+        
+        // Attempt to identify which human is speaking
+        val speakingHuman = identifySpeakingHuman(humans, phrase)
+        
+        // If we identified a speaking human and it's different from current focus
+        speakingHuman?.let { human ->
+            // Update last speaking human and timestamp
+            lastSpeakingHuman = human
+            lastSpeechTimestamp = currentTimestamp
+            
+            // Only switch engagement if:
+            // 1. Different from current engaged human
+            // 2. Enough time has passed since last switch (prevent rapid switching)
+            if (human != currentEngagedHuman && 
+                (currentTimestamp - lastSpeechTimestamp > SPEAKER_SWITCH_DELAY_MS)) {
+                
+                Log.i(TAG, "Switching focus to new speaking human")
+                engageWithSpeakingHuman(human)
+            }
+        }
+    }
+    
+    /**
+     * Identify which human is speaking based on engagement cues
+     */
+    private fun identifySpeakingHuman(humans: List<Human>, phrase: Phrase): Human? {
+        // This is a best-effort approach as the SDK doesn't directly tell us which human is speaking
+        
+        if (humans.isNotEmpty()) {
+            // If there's only one fully engaged human, that's likely our speaker
+            if (humans.size == 1) {
+                return humans.first()
+            }
+            
+            // If multiple engaged humans, use the one with higher excitement
+            // (people often get more animated when speaking)
+            val excitedHuman = humans.maxBy {
+                if (it.emotion.excitement == ExcitementState.EXCITED) 3
+                else if (it.emotion.excitement == ExcitementState.CALM) 2
+                else 1
+            }
+            
+            if (excitedHuman != null) {
+                return excitedHuman
+            }
+        }
+        
+        // If no strong indicators, default to first human or current engaged human
+        return currentEngagedHuman ?: humans.firstOrNull()
+    }
+    
+    /**
+     * Update the engagement policy for the current engagement without stopping it
+     */
+    private fun updateEngagementPolicy(policy: EngagementPolicy) {
+        if (!isEngagementRunning || engageHumanAction == null) {
+            return
+        }
+        
+        try {
+            // Only update if policy is different from current
+            if (currentEngagementPolicy != policy) {
+                Log.i(TAG, "Updating engagement policy from $currentEngagementPolicy to $policy")
+                
+                // Set the new policy on the existing engagement action
+                engageHumanAction?.engagementPolicy = policy
+                
+                // Update current policy
+                currentEngagementPolicy = policy
+            }
+        } catch (e: Exception) {
+            Log.e(TAG, "Error updating engagement policy: ${e.message}", e)
+        }
+    }
+    
+    /**
+     * Engage with a human who is speaking with stronger engagement
+     */
+    private fun engageWithSpeakingHuman(human: Human) {
+        // Use STRONG engagement policy for speaking humans
+        currentEngagementPolicy = EngagementPolicy.STRICT
+        
+        // Send information about the speaker change to WebSocket server
+        sendSpeakerChangeEvent(human)
+    }
+    
+    /**
+     * Send speaker change event to WebSocket server
+     */
+    private fun sendSpeakerChangeEvent(human: Human) {
+        try {
+            val statusJson = JSONObject().apply {
+                put("type", "speaker_change")
+                put("gender", human.estimatedGender.toString())
+                put("age", human.estimatedAge.years)
+                put("emotion", human.emotion.pleasure.toString())
+                put("timestamp", System.currentTimeMillis())
+            }
+            
+            webSocketClient?.sendMessage(statusJson.toString())
+        } catch (e: Exception) {
+            Log.e(TAG, "Error sending speaker change event: ${e.message}", e)
+        }
+    }
+    
+    /**
+     * Clean up conversation tracking resources
+     */
+    private fun cleanupConversationTracking() {
+        // Cancel speech timeout timer
+        speechTimeoutTask?.cancel()
+        speechTimeoutTimer?.purge()
+        speechTimeoutTimer = null
+        speechTimeoutTask = null
+        
+        // Remove listeners
+        conversationStatus?.removeAllOnHeardListeners()
+        conversationStatus?.removeAllOnSayingChangedListeners()
+        conversationStatus = null
+        isSpeechDetectionActive = false
+        Log.i(TAG, "Conversation tracking cleaned up")
+    }
+    
     /**
      * Process the list of humans detected around the robot
      */
@@ -389,14 +618,14 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
             return
         } else {
             // Found at least one engaged human
-            engageWithHuman(humans.first())
+            engageWithHuman(humans.first(), EngagementPolicy.STRICT)
         }
     }
     
     /**
      * Engage with a specific human
      */
-    private fun engageWithHuman(human: Human) {
+    private fun engageWithHuman(human: Human, policy: EngagementPolicy = EngagementPolicy.STRICT) {
         val ctx = qiContext ?: return
         
         try {
@@ -409,13 +638,13 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
             // Debug additional information about the human
             Log.d(TAG, "Engaging with human: attention=${human.attention}, excitement=${human.emotion?.excitement}")
             
-            // Build the engage human action
+            // Build the engage human action with specified policy
             engageHumanAction = EngageHumanBuilder.with(ctx)
                 .withHuman(human)
                 .build()
             
             // Run the engagement
-            Log.i(TAG, "Starting engagement with human")
+            Log.i(TAG, "Starting engagement with human using policy: $policy")
             isEngagementRunning = true
             
             // Run engagement asynchronously
@@ -578,4 +807,49 @@ class MainActivity : RobotActivity(), RobotLifecycleCallbacks,
             sendSpeakingStatus("error", text)
         }
     }
+
+    /**
+     * Set up the WebSocket toggle switch
+     */
+    private fun setupWebSocketToggle() {
+        val webSocketToggle = findViewById<Switch>(R.id.webSocketToggle)
+        
+        // Set initial state (disabled by default)
+        webSocketToggle.isChecked = isWebSocketEnabled
+        
+        // Add listener for toggle changes
+        webSocketToggle.setOnCheckedChangeListener { _, isChecked ->
+            isWebSocketEnabled = isChecked
+            
+            if (isChecked) {
+                // WebSocket enabled - connect if we have QiContext
+                Log.i(TAG, "WebSocket connection enabled")
+                updateStatus("WebSocket connection enabled")
+                
+                qiContext?.let {
+                    connectWebSocket()
+                }
+            } else {
+                // WebSocket disabled - disconnect if connected
+                Log.i(TAG, "WebSocket connection disabled")
+                updateStatus("WebSocket connection disabled")
+                
+                disconnectWebSocket()
+            }
+            
+            // Save the setting to preferences
+            getPreferences(Context.MODE_PRIVATE).edit()
+                .putBoolean("websocket_enabled", isChecked)
+                .apply()
+        }
+        
+        // Restore previous state if saved (but keep disabled by default)
+        val savedEnabled = getPreferences(Context.MODE_PRIVATE)
+            .getBoolean("websocket_enabled", false)
+        
+        if (savedEnabled) {
+            webSocketToggle.isChecked = true
+            // Toggle state will be handled by the listener
+        }
+    }
 }
\ No newline at end of file
diff --git a/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt b/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt
index 5522ed9..03f3fbd 100644
--- a/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt
+++ b/app/src/main/java/com/example/peppertest/websocket/PepperWebSocketClient.kt
@@ -178,6 +178,10 @@ class PepperWebSocketClient(
                                 Log.d(TAG, "Received command: ${json.getString("action")}")
                                 commandListener.onCommandReceived(json)
                             }
+                            "speech" -> {
+                                Log.d(TAG, "Received speech command: ${json.optString("action", "")}")
+                                commandListener.onCommandReceived(json)
+                            }
                             "face_detection" -> {
                                 Log.d(TAG, "Received face detection command: ${json.optString("action", "")}")
                                 commandListener.onCommandReceived(json)
@@ -189,6 +193,8 @@ class PepperWebSocketClient(
                             }
                             else -> {
                                 Log.d(TAG, "Received unknown message type: $type")
+                                // Forward to command listener anyway for future compatibility
+                                commandListener.onCommandReceived(json)
                             }
                         }
                     } else {
diff --git a/app/src/main/res/layout/activity_main.xml b/app/src/main/res/layout/activity_main.xml
index d6e14c0..74b01e4 100644
--- a/app/src/main/res/layout/activity_main.xml
+++ b/app/src/main/res/layout/activity_main.xml
@@ -13,7 +13,7 @@
         android:layout_margin="16dp"
         android:text="Tap a button to play an animation"
         android:textSize="18sp"
-        app:layout_constraintBottom_toTopOf="@id/buttonsContainer"
+        app:layout_constraintBottom_toTopOf="@id/webSocketContainer"
         app:layout_constraintLeft_toLeftOf="parent"
         app:layout_constraintRight_toRightOf="parent"
         app:layout_constraintTop_toTopOf="parent" />
@@ -28,6 +28,33 @@
         app:layout_constraintRight_toRightOf="parent"
         app:layout_constraintTop_toTopOf="parent" />
 
+    <!-- WebSocket Toggle Container -->
+    <LinearLayout
+        android:id="@+id/webSocketContainer"
+        android:layout_width="0dp"
+        android:layout_height="wrap_content"
+        android:layout_margin="16dp"
+        android:orientation="horizontal"
+        app:layout_constraintBottom_toTopOf="@id/buttonsContainer"
+        app:layout_constraintLeft_toLeftOf="parent"
+        app:layout_constraintRight_toRightOf="parent"
+        app:layout_constraintTop_toBottomOf="@id/responseTextView">
+
+        <TextView
+            android:layout_width="0dp"
+            android:layout_height="wrap_content"
+            android:layout_weight="1"
+            android:text="WebSocket Connection"
+            android:textSize="16sp" />
+
+        <Switch
+            android:id="@+id/webSocketToggle"
+            android:layout_width="wrap_content"
+            android:layout_height="wrap_content"
+            android:checked="false"
+            android:text="" />
+    </LinearLayout>
+    
     <LinearLayout
         android:id="@+id/buttonsContainer"
         android:layout_width="0dp"
@@ -37,7 +64,7 @@
         app:layout_constraintBottom_toBottomOf="parent"
         app:layout_constraintLeft_toLeftOf="parent"
         app:layout_constraintRight_toRightOf="parent"
-        app:layout_constraintTop_toBottomOf="@id/responseTextView">
+        app:layout_constraintTop_toBottomOf="@id/webSocketContainer">
 
         <Button
             android:id="@+id/raiseHandsButton"
diff --git a/pepper_vision/requirements.txt b/pepper_vision/requirements.txt
index 0825f52..d7e29cb 100644
--- a/pepper_vision/requirements.txt
+++ b/pepper_vision/requirements.txt
@@ -4,4 +4,12 @@ websockets>=11.0.2
 opencv-python>=4.7.0
 numpy>=1.24.2
 python-dotenv>=1.0.0
-pillow>=9.0.0 
\ No newline at end of file
+pillow>=9.0.0
+# Speech processing dependencies
+flask>=2.3.3
+soundfile>=0.12.1
+requests>=2.31.0
+openai-whisper>=20231117
+pydub>=0.25.1
+torch>=2.0.0
+ffmpeg-python>=0.2.0 
\ No newline at end of file
diff --git a/pepper_vision/server.py b/pepper_vision/server.py
index fb7412d..b123fce 100644
--- a/pepper_vision/server.py
+++ b/pepper_vision/server.py
@@ -9,9 +9,20 @@ import websockets
 import cv2
 import numpy as np
 import uuid
+from aiohttp import web
+import aiohttp
+from speech_processor import SpeechProcessor
+import io
+from tempfile import NamedTemporaryFile
+import pyaudio
+import wave
+import threading
+import time
+import queue
 
 # Global settings
 FACE_DETECTION_ENABLED = False
+LOCAL_MIC_ENABLED = False  # Toggle for local microphone
 
 # Check websockets version and add version-specific imports
 WEBSOCKETS_VERSION = websockets.__version__
@@ -47,13 +58,350 @@ logger = logging.getLogger(__name__)
 os.makedirs("logs", exist_ok=True)
 
 class PepperServer:
-    def __init__(self, host="0.0.0.0", port=5001):
+    def __init__(self, host="0.0.0.0", port=5002):
         self.host = host
         self.port = port
         self.clients = set()
         self.pepper_connection = None
         self.latest_frame = None
         self.latest_face_data = None
+        
+        # Initialize the speech processor
+        self.speech_processor = SpeechProcessor()
+        
+        # Initialize the web app for HTTP API
+        self.app = web.Application()
+        self.setup_routes()
+        
+        # Microphone recording settings
+        self.mic_thread = None
+        self.stop_mic_recording = threading.Event()
+        self.mic_recording = False
+        
+        # Message queue for thread-safe communication
+        self.message_queue = queue.Queue()
+        
+        # Flag to track if message processing is active
+        self.processing_messages = False
+    
+    def setup_routes(self):
+        """Set up the routes for the HTTP API"""
+        self.app.router.add_post('/api/speech', self.handle_speech_api)
+        self.app.router.add_post('/api/receive', self.handle_receive_api)
+        self.app.router.add_post('/api/toggle_mic', self.handle_toggle_mic_api)
+    
+    async def handle_speech_api(self, request):
+        """
+        Handle POST requests to the /api/speech endpoint.
+        This endpoint accepts audio files and processes them using the speech processor.
+        """
+        try:
+            # Check if the request has multipart form data
+            if not request.content_type.startswith('multipart/'):
+                return web.json_response({"error": "Expected multipart form data"}, status=400)
+            
+            # Parse the multipart form data
+            reader = await request.multipart()
+            
+            # Get the audio file field
+            field = await reader.next()
+            if field is None or field.name != 'audio':
+                return web.json_response({"error": "No audio file provided"}, status=400)
+            
+            # Create a temporary file to save the audio
+            with NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
+                temp_path = temp_file.name
+                
+                # Read the audio file in chunks and write to temp file
+                while True:
+                    chunk = await field.read_chunk()
+                    if not chunk:
+                        break
+                    temp_file.write(chunk)
+            
+            # Create a file-like object from the temp file for the speech processor
+            class AudioFile:
+                def __init__(self, path):
+                    self.path = path
+                
+                def save(self, path):
+                    with open(self.path, 'rb') as src, open(path, 'wb') as dst:
+                        dst.write(src.read())
+            
+            audio_file = AudioFile(temp_path)
+            
+            # Process the audio file
+            result = self.speech_processor.process_audio_file(audio_file)
+            
+            # Clean up the temp file
+            os.unlink(temp_path)
+            
+            # Check if we have an LLM response
+            if "llm_response" in result and self.pepper_connection:
+                # Send the response to Pepper for TTS
+                await self.pepper_connection.send(json.dumps({
+                    "type": "speech",
+                    "action": "say",
+                    "text": result["llm_response"]
+                }))
+            
+            # Return the result
+            return web.json_response(result)
+            
+        except Exception as e:
+            logger.error(f"Error processing speech API request: {e}")
+            return web.json_response({"error": str(e)}, status=500)
+    
+    async def handle_receive_api(self, request):
+        """
+        Handle POST requests to the /api/receive endpoint.
+        This endpoint accepts JSON with response and input_text fields.
+        """
+        try:
+            # Get the JSON data
+            data = await request.json()
+            
+            # Extract the response and input text
+            response_text = data.get("response")
+            input_text = data.get("input_text")
+            
+            if not response_text:
+                return web.json_response({"error": "No response text provided"}, status=400)
+            
+            logger.info(f"Received response via API: '{response_text}' (input: '{input_text}')")
+            
+            # If Pepper is connected, send the response for TTS
+            if self.pepper_connection:
+                await self.pepper_connection.send(json.dumps({
+                    "type": "speech",
+                    "action": "say",
+                    "text": response_text
+                }))
+                return web.json_response({"status": "success", "message": "Response sent to Pepper"})
+            else:
+                return web.json_response({"status": "error", "message": "Pepper not connected"}, status=503)
+                
+        except Exception as e:
+            logger.error(f"Error processing receive API request: {e}")
+            return web.json_response({"error": str(e)}, status=500)
+    
+    async def handle_toggle_mic_api(self, request):
+        """
+        Handle POST requests to the /api/toggle_mic endpoint.
+        This endpoint toggles the local microphone recording.
+        """
+        global LOCAL_MIC_ENABLED
+        
+        try:
+            data = await request.json()
+            enable = data.get("enable")
+            
+            if enable is None:
+                # Toggle if not specified
+                enable = not LOCAL_MIC_ENABLED
+            
+            if enable:
+                if not self.mic_recording:
+                    # Start microphone recording
+                    self.start_microphone_recording()
+                    LOCAL_MIC_ENABLED = True
+                    return web.json_response({"status": "success", "mic_enabled": True})
+                else:
+                    return web.json_response({"status": "info", "message": "Microphone already recording", "mic_enabled": True})
+            else:
+                if self.mic_recording:
+                    # Stop microphone recording
+                    self.stop_microphone_recording()
+                    LOCAL_MIC_ENABLED = False
+                    return web.json_response({"status": "success", "mic_enabled": False})
+                else:
+                    return web.json_response({"status": "info", "message": "Microphone already stopped", "mic_enabled": False})
+                
+        except Exception as e:
+            logger.error(f"Error toggling microphone: {e}")
+            return web.json_response({"error": str(e)}, status=500)
+    
+    def start_microphone_recording(self):
+        """Start recording from the local microphone"""
+        if self.mic_recording:
+            logger.warning("Microphone recording already active")
+            return
+            
+        # Reset the stop event
+        self.stop_mic_recording.clear()
+        
+        # Start a new thread for microphone recording
+        self.mic_thread = threading.Thread(target=self._microphone_recording_thread)
+        self.mic_thread.daemon = True
+        self.mic_thread.start()
+        
+        self.mic_recording = True
+        logger.info("Started local microphone recording")
+    
+    def stop_microphone_recording(self):
+        """Stop recording from the local microphone"""
+        if not self.mic_recording:
+            logger.warning("No microphone recording to stop")
+            return
+            
+        # Signal the recording thread to stop
+        self.stop_mic_recording.set()
+        
+        # Wait for the thread to finish
+        if self.mic_thread:
+            self.mic_thread.join(timeout=2.0)
+            self.mic_thread = None
+        
+        self.mic_recording = False
+        logger.info("Stopped local microphone recording")
+    
+    def _microphone_recording_thread(self):
+        """Thread function for recording from the microphone"""
+        try:
+            # PyAudio setup
+            CHUNK = 1024
+            FORMAT = pyaudio.paInt16
+            CHANNELS = 1
+            RATE = 16000
+            RECORD_SECONDS = 5  # Record in 5-second chunks
+            
+            p = pyaudio.PyAudio()
+            
+            # Open stream
+            stream = p.open(format=FORMAT,
+                            channels=CHANNELS,
+                            rate=RATE,
+                            input=True,
+                            frames_per_buffer=CHUNK)
+            
+            logger.info("Microphone stream opened, listening...")
+            
+            while not self.stop_mic_recording.is_set():
+                # Record audio for RECORD_SECONDS
+                frames = []
+                
+                for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
+                    # Check if we should stop
+                    if self.stop_mic_recording.is_set():
+                        break
+                        
+                    # Read audio data
+                    data = stream.read(CHUNK, exception_on_overflow=False)
+                    frames.append(data)
+                
+                # Process the recording if we have enough data
+                if len(frames) > 0:
+                    # Save to temp file
+                    with NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
+                        temp_path = temp_file.name
+                    
+                    wf = wave.open(temp_path, 'wb')
+                    wf.setnchannels(CHANNELS)
+                    wf.setsampwidth(p.get_sample_size(FORMAT))
+                    wf.setframerate(RATE)
+                    wf.writeframes(b''.join(frames))
+                    wf.close()
+                    
+                    # Create a file-like object for the speech processor
+                    class AudioFile:
+                        def __init__(self, path):
+                            self.path = path
+                        
+                        def save(self, path):
+                            with open(self.path, 'rb') as src, open(path, 'wb') as dst:
+                                dst.write(src.read())
+                    
+                    audio_file = AudioFile(temp_path)
+                    
+                    # Process the audio in a separate thread to not block microphone recording
+                    threading.Thread(
+                        target=self._process_microphone_audio,
+                        args=(audio_file,),
+                        daemon=True
+                    ).start()
+            
+            # Clean up when stopped
+            stream.stop_stream()
+            stream.close()
+            p.terminate()
+            logger.info("Microphone stream closed")
+            
+        except Exception as e:
+            logger.error(f"Error in microphone recording thread: {e}")
+            self.mic_recording = False
+    
+    def _process_microphone_audio(self, audio_file):
+        """Process audio from the local microphone"""
+        try:
+            # Process the audio
+            result = self.speech_processor.process_audio_file(audio_file)
+            
+            # Clean up temp file
+            if hasattr(audio_file, 'path') and os.path.exists(audio_file.path):
+                os.unlink(audio_file.path)
+            
+            # If no speech detected, just return
+            if result.get("status") == "no_speech" or "error" in result:
+                return
+            
+            # Log the result
+            logger.info(f"Local microphone speech detected: {result.get('input_text')}")
+            
+            # If we have an LLM response, queue it for sending to Pepper
+            if "llm_response" in result:
+                message = {
+                    "type": "speech",
+                    "action": "say",
+                    "text": result["llm_response"]
+                }
+                
+                # Add message to the queue for the main thread to process
+                self.message_queue.put(message)
+                logger.info(f"Queued message for Pepper: {result.get('llm_response')}")
+                
+        except Exception as e:
+            logger.error(f"Error processing microphone audio: {e}")
+            logger.exception("Full traceback:")
+    
+    async def process_message_queue(self):
+        """Process messages from the queue and send them to Pepper"""
+        self.processing_messages = True
+        
+        try:
+            while True:
+                # Check if there's a message in the queue
+                try:
+                    # Non-blocking get with timeout
+                    message = self.message_queue.get(block=False)
+                    
+                    # Send the message if Pepper is connected
+                    if self.pepper_connection:
+                        try:
+                            await self.pepper_connection.send(json.dumps(message))
+                            logger.info(f"Sent message to Pepper: {message.get('text')}")
+                        except Exception as e:
+                            logger.error(f"Error sending message to Pepper: {e}")
+                    else:
+                        logger.warning(f"Pepper not connected, couldn't send: {message.get('text')}")
+                    
+                    # Mark the task as done
+                    self.message_queue.task_done()
+                    
+                except queue.Empty:
+                    # No messages in queue, wait before checking again
+                    pass
+                
+                # Small delay to prevent CPU hogging
+                await asyncio.sleep(0.1)
+                
+        except asyncio.CancelledError:
+            # Task was cancelled, clean up
+            self.processing_messages = False
+            logger.info("Message queue processing stopped")
+        except Exception as e:
+            self.processing_messages = False
+            logger.error(f"Error in message queue processing: {e}")
+            logger.exception("Full traceback:")
     
     def get_ip_address(self):
         """Get the local IP address for network connectivity"""
@@ -76,10 +424,17 @@ class PepperServer:
         # Store the Pepper connection
         self.pepper_connection = websocket
         
+        # Start processing the message queue if not already running
+        if not self.processing_messages:
+            asyncio.create_task(self.process_message_queue())
+        
         try:
             async for message in websocket:
                 # If message is from Pepper, it's a camera frame or JSON data
                 if isinstance(message, bytes):
+                    # Check if it's an audio message (we need to implement a protocol for this)
+                    # For now, assume all binary messages are camera frames
+                    
                     # Store the latest frame
                     self.latest_frame = message
                     
@@ -116,6 +471,25 @@ class PepperServer:
                                     *[client.send(message) for client in self.clients],
                                     return_exceptions=True
                                 )
+                        # If it's an audio message with text (from speech recognition on Pepper)
+                        elif data.get("type") == "speech" and data.get("action") == "recognized":
+                            text_input = data.get("text")
+                            if text_input:
+                                logger.info(f"Received recognized speech from Pepper: '{text_input}'")
+                                
+                                # Process with LLM
+                                llm_response = self.speech_processor.get_llm_response(
+                                    text_input, 
+                                    prompt_prefix="IMPORTANT: Be extremely brief. Respond with only 1-2 very short sentences. No greetings or explanations. Question: "
+                                )
+                                
+                                if llm_response:
+                                    # Send the response back to Pepper for TTS
+                                    await websocket.send(json.dumps({
+                                        "type": "speech",
+                                        "action": "say",
+                                        "text": llm_response
+                                    }))
                         # Other types of messages
                         else:
                             logger.debug(f"Received other message type: {data.get('type')}")
@@ -180,11 +554,61 @@ class PepperServer:
                                     logger.info("Forwarding face detection disable command to Pepper")
                                     await self.pepper_connection.send(json.dumps(command))
                         
+                        # Handle speech commands
+                        elif command.get("type") == "speech":
+                            action = command.get("action")
+                            
+                            # Command to make Pepper say something
+                            if action == "say" and command.get("text"):
+                                text = command.get("text")
+                                logger.info(f"Received say command: '{text}'")
+                                
+                                # Forward to Pepper if connected
+                                if self.pepper_connection:
+                                    logger.info(f"Forwarding say command to Pepper: '{text}'")
+                                    await self.pepper_connection.send(json.dumps(command))
+                                else:
+                                    logger.warning("Cannot forward say command: Pepper not connected")
+                            
+                            # Command to start listening
+                            elif action == "start_listening":
+                                logger.info("Received start_listening command")
+                                
+                                # Forward to Pepper if connected
+                                if self.pepper_connection:
+                                    logger.info("Forwarding start_listening command to Pepper")
+                                    await self.pepper_connection.send(json.dumps(command))
+                                else:
+                                    logger.warning("Cannot forward start_listening command: Pepper not connected")
+                        
                         # Forward other commands to Pepper if connected
                         elif self.pepper_connection:
                             await self.pepper_connection.send(json.dumps(command))
                         else:
                             logger.warning("Cannot forward command: Pepper not connected")
+                    # Handle binary messages (likely audio data)
+                    elif isinstance(message, bytes):
+                        logger.info(f"Received binary data from client {client_id}")
+                        
+                        # Save to temp file
+                        with NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
+                            temp_path = temp_file.name
+                            temp_file.write(message)
+                        
+                        # Create a file-like object
+                        class AudioFile:
+                            def __init__(self, path):
+                                self.path = path
+                            
+                            def save(self, path):
+                                with open(self.path, 'rb') as src, open(path, 'wb') as dst:
+                                    dst.write(src.read())
+                        
+                        audio_file = AudioFile(temp_path)
+                        
+                        # Process asynchronously to not block WebSocket
+                        asyncio.create_task(self.process_client_audio(audio_file, websocket))
+                        
                 except json.JSONDecodeError:
                     logger.warning(f"Received invalid command from client {client_id}")
                 except Exception as e:
@@ -202,6 +626,35 @@ class PepperServer:
             self.clients.remove(websocket)
             logger.info(f"Client {client_id} disconnected, {len(self.clients)} clients remaining")
     
+    async def process_client_audio(self, audio_file, client_websocket):
+        """Process audio data from a client asynchronously"""
+        try:
+            # Process the audio
+            result = self.speech_processor.process_audio_file(audio_file)
+            
+            # Clean up temp file
+            if hasattr(audio_file, 'path') and os.path.exists(audio_file.path):
+                os.unlink(audio_file.path)
+            
+            # Send the result back to the client
+            await client_websocket.send(json.dumps(result))
+            
+            # If we have an LLM response and Pepper is connected, send to Pepper
+            if "llm_response" in result and self.pepper_connection:
+                await self.pepper_connection.send(json.dumps({
+                    "type": "speech",
+                    "action": "say",
+                    "text": result["llm_response"]
+                }))
+                
+        except Exception as e:
+            logger.error(f"Error processing client audio: {e}")
+            # Try to send error back to client
+            try:
+                await client_websocket.send(json.dumps({"error": str(e)}))
+            except:
+                pass
+    
     # This handler will be used by the newer websockets library
     async def router(self, websocket, request_uri):
         """Route WebSocket connections based on the request URI"""
@@ -209,17 +662,54 @@ class PepperServer:
         pass
     
     async def start_server(self):
-        """Start the WebSocket server"""
-        raise NotImplementedError("Use main() directly instead of start_server method")
+        """Start the WebSocket server and HTTP API server"""
+        ip_address = self.get_ip_address()
+        
+        # Create a runner for the web app
+        runner = web.AppRunner(self.app)
+        await runner.setup()
+        
+        # Create a site for the app
+        site = web.TCPSite(runner, self.host, self.port)
+        
+        # Start the HTTP server
+        await site.start()
+        logger.info(f"HTTP API server started on http://{ip_address}:{self.port}")
+        
+        # Start the WebSocket server
+        async def websocket_handler(websocket, path):
+            # Determine if this is a Pepper connection or a client
+            if path == "/pepper":
+                await self.handle_pepper(websocket)
+            else:
+                await self.handle_client(websocket)
+        
+        # Get the version of websockets
+        major_version = int(WEBSOCKETS_VERSION.split('.')[0])
+        
+        if major_version >= 10:
+            # For newer versions of websockets
+            async with websockets.serve(websocket_handler, self.host, self.port + 1):
+                logger.info(f"WebSocket server started on ws://{ip_address}:{self.port + 1}")
+                logger.info(f"Pepper robot should connect to: ws://{ip_address}:{self.port + 1}/pepper")
+                logger.info(f"Webcam clients should connect to: ws://{ip_address}:{self.port + 1}")
+                
+                # Keep the server running
+                await asyncio.Future()
+        else:
+            # For older versions of websockets
+            server = await websockets.serve(websocket_handler, self.host, self.port + 1)
+            logger.info(f"WebSocket server started on ws://{ip_address}:{self.port + 1}")
+            logger.info(f"Pepper robot should connect to: ws://{ip_address}:{self.port + 1}/pepper")
+            logger.info(f"Webcam clients should connect to: ws://{ip_address}:{self.port + 1}")
+            
+            # Keep the server running
+            await asyncio.Future()
 
-# Function to handle connection debugging and incoming requests
-async def process_request(request, path=None):
-    """
-    Custom request handler for websockets 15.0.1
-    In newer versions, this receives a Request object as the first parameter
-    """
-    # This function is no longer used
-    pass
+        # Start local microphone if enabled
+        global LOCAL_MIC_ENABLED
+        if LOCAL_MIC_ENABLED:
+            self.start_microphone_recording()
 
 async def main():
     server = PepperServer()
@@ -268,13 +758,27 @@ async def main():
                 logger.error(f"Unexpected error in connection handler: {e}")
                 logger.exception("Full traceback:")
         
+        # Get the ip address for display purposes
         ip_address = server.get_ip_address()
-        logger.info(f"Starting WebSocket server on ws://{ip_address}:{server.port}")
-        logger.info(f"Pepper robot should connect to: ws://{ip_address}:{server.port}/pepper")
-        logger.info(f"Webcam clients should connect to: ws://{ip_address}:{server.port}")
+        
+        # Start the HTTP API server
+        runner = web.AppRunner(server.app)
+        await runner.setup()
+        site = web.TCPSite(runner, server.host, server.port)
+        await site.start()
+        
+        logger.info(f"HTTP API server started on http://{ip_address}:{server.port}")
+        logger.info(f"  - Speech API endpoint: http://{ip_address}:{server.port}/api/speech")
+        logger.info(f"  - Receive API endpoint: http://{ip_address}:{server.port}/api/receive")
+        
+        # Start WebSocket server on a different port (port+1)
+        ws_port = server.port + 1
+        logger.info(f"Starting WebSocket server on ws://{ip_address}:{ws_port}")
+        logger.info(f"Pepper robot should connect to: ws://{ip_address}:{ws_port}/pepper")
+        logger.info(f"Webcam clients should connect to: ws://{ip_address}:{ws_port}")
         
         # Use the simplest form of server creation, which should work across versions
-        server_instance = await websockets.serve(handler, server.host, server.port)
+        server_instance = await websockets.serve(handler, server.host, ws_port)
         logger.info(f"Server started successfully using websockets {WEBSOCKETS_VERSION}")
         
         # Keep the server running
diff --git a/pepper_vision/start.sh b/pepper_vision/start.sh
index bb6e6af..68a1367 100755
--- a/pepper_vision/start.sh
+++ b/pepper_vision/start.sh
@@ -30,15 +30,81 @@ if [ $? -ne 0 ]; then
     fi
 fi
 
+# Check for speech processing dependencies
+echo "Checking for speech processing dependencies..."
+python3 -c "import whisper; print('Whisper installed')" 2>/dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: openai-whisper package is missing. Speech-to-text functionality will not work."
+    echo "Install it with: pip install openai-whisper"
+    read -p "Continue anyway? (y/n) " -n 1 -r
+    echo
+    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
+        exit 1
+    fi
+fi
+
+python3 -c "import torch; print('PyTorch installed')" 2>/dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: torch package is missing. Voice activity detection will not work."
+    echo "Install it with: pip install torch"
+fi
+
+python3 -c "import soundfile; print('SoundFile installed')" 2>/dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: soundfile package is missing. Audio processing will not work."
+    echo "Install it with: pip install soundfile"
+fi
+
+python3 -c "import pydub; print('PyDub installed')" 2>/dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: pydub package is missing. Audio format conversion will not work."
+    echo "Install it with: pip install pydub"
+fi
+
+python3 -c "import pyaudio; print('PyAudio installed')" 2>/dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: pyaudio package is missing. Local microphone input will not work."
+    echo "Install it with: pip install pyaudio"
+    echo "Note: On Ubuntu/Debian, you may need to install portaudio first: sudo apt-get install portaudio19-dev"
+fi
+
+python3 -c "import aiohttp; print('aiohttp installed')" 2>/dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: aiohttp package is missing. HTTP API will not work."
+    echo "Install it with: pip install aiohttp"
+    read -p "Continue anyway? (y/n) " -n 1 -r
+    echo
+    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
+        exit 1
+    fi
+fi
+
+# Check if Ollama is running
+echo "Checking if Ollama is running..."
+curl -s http://localhost:11434/api/version > /dev/null
+if [ $? -ne 0 ]; then
+    echo "Warning: Ollama does not appear to be running or is not accessible at http://localhost:11434"
+    echo "LLM functionality will not work. Please start Ollama before using speech features."
+    read -p "Continue anyway? (y/n) " -n 1 -r
+    echo
+    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
+        exit 1
+    fi
+else
+    echo "Ollama is running. Good!"
+fi
+
 # Get the local IP address
 LOCAL_IP=$(hostname -I | awk '{print $1}')
 echo "Local IP address: $LOCAL_IP"
 echo "Use this IP address to connect from Pepper robot and clients"
-echo "Pepper robot should connect to: ws://$LOCAL_IP:5001/pepper"
+echo "Pepper robot should connect to: ws://$LOCAL_IP:5003/pepper"
 echo "Web clients can view at: http://$LOCAL_IP:8000"
+echo "HTTP API endpoint: http://$LOCAL_IP:5002/api/speech"
+echo "Receive API endpoint: http://$LOCAL_IP:5002/api/receive"
 
 # Start the WebSocket server in the background
-echo "Starting WebSocket server on port 5001..."
+echo "Starting WebSocket server on port 5002 (WebSocket on 5003)..."
 python3 server.py &
 WS_SERVER_PID=$!
 
@@ -62,4 +128,17 @@ trap cleanup INT
 # Wait for user to press Ctrl+C
 echo ""
 echo "Servers started. Press Ctrl+C to stop."
+echo "--------------------------------------"
+echo "Speech Processing Endpoints:"
+echo "- To send audio for processing: POST http://$LOCAL_IP:5002/api/speech"
+echo "  (Send audio file in multipart/form-data with field name 'audio')"
+echo "- To send a text response to Pepper: POST http://$LOCAL_IP:5002/api/receive"
+echo "  (Send JSON with 'response' and 'input_text' fields)"
+echo "- To toggle local microphone: POST http://$LOCAL_IP:5002/api/toggle_mic"
+echo "  (Send JSON with 'enable' field set to true or false)"
+echo ""
+echo "WebSocket Commands for Speech:"
+echo "- To make Pepper speak: {\"type\": \"speech\", \"action\": \"say\", \"text\": \"Hello!\"}"
+echo "- To start listening: {\"type\": \"speech\", \"action\": \"start_listening\"}"
+echo ""
 wait 
\ No newline at end of file
